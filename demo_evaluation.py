#!/usr/bin/env python3
"""
Demo script for the Multi-Agent Persona System Evaluation Pipeline.

This script demonstrates the evaluation pipeline with a single test query,
showing all the evaluation steps and metrics in action.
"""

import os
import sys
from datetime import datetime

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from evaluation_pipeline import EvaluationPipeline, EvaluationConfig

def run_demo():
    """Run a demonstration of the evaluation pipeline."""
    
    print("ğŸ­ Multi-Agent Persona System Evaluation Demo")
    print("=" * 60)
    print("This demo will evaluate the system with a single test query")
    print("and show all evaluation metrics in action.")
    print()
    
    # Check environment
    if not os.getenv("GOOGLE_API_KEY"):
        print("âŒ GOOGLE_API_KEY environment variable not set!")
        print("Please set your Google API key before running the demo.")
        print("\nExample:")
        print("export GOOGLE_API_KEY='your-api-key-here'")
        return False
    
    # Demo query
    demo_query = "TÃ¼rk kÃ¼ltÃ¼rel kimliÄŸi ve modernleÅŸme arasÄ±ndaki iliÅŸki nasÄ±l deÄŸerlendirilmelidir?"
    
    print(f"ğŸ¯ Demo Query: {demo_query}")
    print()
    
    try:
        # Create evaluation configuration
        config = EvaluationConfig(
            test_queries=[demo_query],
            output_dir="demo_evaluation_results",
            save_detailed_results=True,
            save_summary_table=True,
            enable_ragas=True,
            enable_coherence=True
        )
        
        print("ğŸ”§ Creating evaluation pipeline...")
        pipeline = EvaluationPipeline(config)
        
        print("ğŸš€ Starting evaluation...")
        print("-" * 60)
        
        # Run evaluation
        results = pipeline.run_evaluation()
        
        print("-" * 60)
        print("ğŸ’¾ Saving results...")
        pipeline.save_results()
        
        # Display demo summary
        if results:
            result = results[0]
            print(f"\nğŸ‰ Demo Evaluation Completed Successfully!")
            print("=" * 60)
            
            print(f"ğŸ“ Query: {result.query}")
            print(f"â° Timestamp: {result.timestamp}")
            print()
            
            print("ğŸ“Š EVALUATION METRICS:")
            print("-" * 30)
            
            if result.faithfulness_score is not None:
                print(f"  ğŸ¯ Faithfulness: {result.faithfulness_score:.3f}")
            else:
                print(f"  ğŸ¯ Faithfulness: N/A")
                
            if result.answer_relevancy_score is not None:
                print(f"  ğŸ¯ Answer Relevancy: {result.answer_relevancy_score:.3f}")
            else:
                print(f"  ğŸ¯ Answer Relevancy: N/A")
                
            if result.context_precision_score is not None:
                print(f"  ğŸ¯ Context Precision: {result.context_precision_score:.3f}")
            else:
                print(f"  ğŸ¯ Context Precision: N/A")
                
            if result.context_recall_score is not None:
                print(f"  ğŸ¯ Context Recall: {result.context_recall_score:.3f}")
            else:
                print(f"  ğŸ¯ Context Recall: N/A")
                
            if result.coherence_score is not None:
                print(f"  ğŸ§  Coherence: {result.coherence_score:.3f}")
            else:
                print(f"  ğŸ§  Coherence: N/A")
            
            print()
            print("ğŸ“ SYSTEM RESPONSES:")
            print("-" * 30)
            print(f"  ğŸ“š Sources Found: {len(result.sources)}")
            print(f"  ğŸ­ Erol GÃ¼ngÃ¶r Response: {len(result.erol_response)} characters")
            print(f"  ğŸ­ Cemil MeriÃ§ Response: {len(result.cemil_response)} characters")
            print(f"  ğŸ¤ Synthesized Response: {len(result.synthesized_response)} characters")
            
            if result.errors:
                print()
                print("âš ï¸ ERRORS:")
                print("-" * 30)
                for error in result.errors:
                    print(f"  âŒ {error}")
            else:
                print()
                print("âœ… No errors encountered!")
            
            print()
            print(f"ğŸ“‚ Results saved to: {config.output_dir}/")
            print("   - detailed_results.json (complete evaluation data)")
            print("   - summary_table.csv (metrics summary)")
            
            print()
            print("ğŸ” SAMPLE RESPONSES:")
            print("-" * 30)
            
            # Show truncated responses
            erol_preview = result.erol_response[:200] + "..." if len(result.erol_response) > 200 else result.erol_response
            cemil_preview = result.cemil_response[:200] + "..." if len(result.cemil_response) > 200 else result.cemil_response
            synth_preview = result.synthesized_response[:200] + "..." if len(result.synthesized_response) > 200 else result.synthesized_response
            
            print(f"ğŸ“– Erol GÃ¼ngÃ¶r: {erol_preview}")
            print()
            print(f"ğŸ“– Cemil MeriÃ§: {cemil_preview}")
            print()
            print(f"ğŸ¤ Synthesized: {synth_preview}")
            
        print()
        print("ğŸŠ Demo completed! Check the output directory for detailed results.")
        return True
        
    except Exception as e:
        print(f"\nâŒ Demo failed: {str(e)}")
        print("Please check your setup and try again.")
        return False

def show_demo_info():
    """Show information about what the demo does."""
    
    print("ğŸ“‹ DEMO INFORMATION")
    print("=" * 60)
    print("This demo will:")
    print("  1. ğŸ¯ Send a test query to the multi-agent system")
    print("  2. ğŸ¤– Run both Erol GÃ¼ngÃ¶r and Cemil MeriÃ§ agents")
    print("  3. ğŸ¤ Generate a synthesized response")
    print("  4. ğŸ“Š Evaluate using RAGAS metrics:")
    print("     - Faithfulness (groundedness in sources)")
    print("     - Answer Relevancy (relevance to query)")
    print("     - Context Precision (quality of retrieved context)")
    print("     - Context Recall (completeness of context)")
    print("  5. ğŸ§  Evaluate coherence using LangChain evaluator")
    print("  6. ğŸ’¾ Save detailed results and summary table")
    print("  7. ğŸ“Š Display formatted results")
    print()
    print("Expected runtime: 2-5 minutes")
    print("Required: GOOGLE_API_KEY environment variable")
    print()

def main():
    """Main function for the demo."""
    
    print("ğŸ­ Multi-Agent Persona Evaluation Demo")
    print("=" * 50)
    
    # Show demo info
    show_demo_info()
    
    # Ask for confirmation
    response = input("Do you want to run the demo? (y/n): ").strip().lower()
    
    if response in ['y', 'yes']:
        print("\nğŸš€ Starting demo...")
        success = run_demo()
        
        if success:
            print("\nâœ¨ Demo completed successfully!")
            print("You can now examine the results in the demo_evaluation_results/ directory.")
        else:
            print("\nğŸ’¥ Demo failed. Please check the error messages above.")
            
    else:
        print("\nğŸ‘‹ Demo cancelled. Run again when you're ready!")

if __name__ == "__main__":
    main() 