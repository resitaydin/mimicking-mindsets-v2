"""
Phase 2: Multi-Agent Orchestration with LangGraph

Bu modÃ¼l, iki persona ajanÄ±nÄ±n (Erol GÃ¼ngÃ¶r ve Cemil MeriÃ§) paralel Ã§alÄ±ÅŸmasÄ±nÄ± koordine eden
Ã§ok-ajanli orkestrasyon sistemini iÃ§erir. LangGraph kullanarak tool node'larÄ± ve conditional
edge'leri destekler.

Bu system ÅŸu bileÅŸenleri iÃ§erir:
- Graph State: TÃ¼m ajanlarÄ±n durumunu takip eden TypedDict
- Persona Agent Nodes: Her persona iÃ§in ayrÄ± node'lar
- Tool Nodes: Her ajanÄ±n tool Ã§aÄŸrÄ±larÄ±nÄ± yÃ¶neten node'lar
- Synthesizer Node: AjanlarÄ±n Ã§Ä±ktÄ±larÄ±nÄ± birleÅŸtiren node
- Memory Node: Sohbet geÃ§miÅŸini gÃ¼ncelleyen node
- LangSmith Tracing: Comprehensive agent execution tracing
"""

import asyncio
from typing import Any, Dict, List, Optional, TypedDict, Annotated, Literal
from dataclasses import dataclass

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

# Import Phase 1 components
from persona_agents import initialize_components, create_persona_agent
from persona_prompts import get_persona_info, list_available_personas

# Import LangSmith tracing
from langsmith_tracing import (
    initialize_tracing, 
    trace_agent_execution, 
    update_agent_trace, 
    complete_agent_trace,
    get_current_agent_status,
    get_realtime_callback
)

# --- Graph State Definition ---

class GraphState(TypedDict):
    """LangGraph iÃ§in durum yÃ¶netimi."""
    user_query: str
    erol_gungor_agent_output: Optional[Dict[str, Any]]
    cemil_meric_agent_output: Optional[Dict[str, Any]]
    synthesized_answer: Optional[str]
    agent_responses: Optional[Dict[str, str]]
    sources: Optional[List[Dict[str, str]]]
    chat_history: Annotated[List[BaseMessage], add_messages]
    # Tracing fields
    session_id: Optional[str]
    erol_trace_id: Optional[str]
    cemil_trace_id: Optional[str]

# --- Node Functions ---

def erol_gungor_agent_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Erol GÃ¼ngÃ¶r ajanÄ±nÄ± Ã§alÄ±ÅŸtÄ±ran node."""
    
    print(f"\nðŸŽ¯ DEBUG: Starting Erol GÃ¼ngÃ¶r agent node")
    print(f"ðŸ” DEBUG: User query: {state['user_query']}")
    
    # Start tracing
    trace_id = trace_agent_execution("Erol GÃ¼ngÃ¶r", state['user_query'])
    
    # Get the agent from config
    erol_agent = config.get("configurable", {}).get("erol_agent")
    if not erol_agent:
        print(f"âŒ DEBUG: Erol GÃ¼ngÃ¶r agent not found in config")
        complete_agent_trace(trace_id, "", "Erol GÃ¼ngÃ¶r ajanÄ± yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")
        return {
            "erol_gungor_agent_output": {"error": "Erol GÃ¼ngÃ¶r ajanÄ± yapÄ±landÄ±rÄ±lmamÄ±ÅŸ"},
            "erol_trace_id": trace_id
        }
    
    try:
        # Update trace status
        update_agent_trace(trace_id, "Sorgu analiz ediliyor...")
        
        # Create message for the agent
        messages = [HumanMessage(content=state["user_query"])]
        print(f"ðŸ’¬ DEBUG: Invoking Erol GÃ¼ngÃ¶r agent")
        
        # Update trace for agent invocation
        update_agent_trace(trace_id, "Ajan Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        # Get real-time callback for this agent
        callback = get_realtime_callback("Erol GÃ¼ngÃ¶r", trace_id)
        
        # Invoke the agent with real-time callback (if available)
        if callback:
            result = erol_agent.invoke({"messages": messages}, config={"callbacks": [callback]})
        else:
            result = erol_agent.invoke({"messages": messages})
        print(f"âœ… DEBUG: Erol GÃ¼ngÃ¶r agent completed successfully")
        print(f"ðŸ“Š DEBUG: Agent returned {len(result.get('messages', []))} messages")
        
        # Extract response for tracing
        response_text = ""
        if result and "messages" in result and result["messages"]:
            last_message = result["messages"][-1]
            response_text = last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        # Complete trace
        complete_agent_trace(trace_id, response_text)
        
        return {
            "erol_gungor_agent_output": result,
            "erol_trace_id": trace_id
        }
        
    except Exception as e:
        print(f"âŒ DEBUG: Error in Erol GÃ¼ngÃ¶r agent node: {str(e)}")
        complete_agent_trace(trace_id, "", str(e))
        return {
            "erol_gungor_agent_output": {"error": f"Erol GÃ¼ngÃ¶r ajanÄ± hatasÄ±: {str(e)}"},
            "erol_trace_id": trace_id
        }

def cemil_meric_agent_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Cemil MeriÃ§ ajanÄ±nÄ± Ã§alÄ±ÅŸtÄ±ran node."""
    
    print(f"\nðŸŽ¯ DEBUG: Starting Cemil MeriÃ§ agent node")
    print(f"ðŸ” DEBUG: User query: {state['user_query']}")
    
    # Start tracing
    trace_id = trace_agent_execution("Cemil MeriÃ§", state['user_query'])
    
    # Get the agent from config
    cemil_agent = config.get("configurable", {}).get("cemil_agent")
    if not cemil_agent:
        print(f"âŒ DEBUG: Cemil MeriÃ§ agent not found in config")
        complete_agent_trace(trace_id, "", "Cemil MeriÃ§ ajanÄ± yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")
        return {
            "cemil_meric_agent_output": {"error": "Cemil MeriÃ§ ajanÄ± yapÄ±landÄ±rÄ±lmamÄ±ÅŸ"},
            "cemil_trace_id": trace_id
        }
    
    try:
        # Update trace status
        update_agent_trace(trace_id, "Felsefi Ã§erÃ§eve oluÅŸturuluyor...")
        
        # Create message for the agent
        messages = [HumanMessage(content=state["user_query"])]
        print(f"ðŸ’¬ DEBUG: Invoking Cemil MeriÃ§ agent")
        
        # Update trace for agent invocation
        update_agent_trace(trace_id, "Ajan Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        # Get real-time callback for this agent
        callback = get_realtime_callback("Cemil MeriÃ§", trace_id)
        
        # Invoke the agent with real-time callback (if available)
        if callback:
            result = cemil_agent.invoke({"messages": messages}, config={"callbacks": [callback]})
        else:
            result = cemil_agent.invoke({"messages": messages})
        print(f"âœ… DEBUG: Cemil MeriÃ§ agent completed successfully")
        print(f"ðŸ“Š DEBUG: Agent returned {len(result.get('messages', []))} messages")
        
        # Extract response for tracing
        response_text = ""
        if result and "messages" in result and result["messages"]:
            last_message = result["messages"][-1]
            response_text = last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        # Complete trace
        complete_agent_trace(trace_id, response_text)
        
        return {
            "cemil_meric_agent_output": result,
            "cemil_trace_id": trace_id
        }
        
    except Exception as e:
        print(f"âŒ DEBUG: Error in Cemil MeriÃ§ agent node: {str(e)}")
        complete_agent_trace(trace_id, "", str(e))
        return {
            "cemil_meric_agent_output": {"error": f"Cemil MeriÃ§ ajanÄ± hatasÄ±: {str(e)}"},
            "cemil_trace_id": trace_id
        }

def join_agents_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Ä°ki ajanÄ±n tamamlanmasÄ±nÄ± bekleyen ara node."""
    
    print(f"\nðŸ”„ DEBUG: Join node - checking if both agents completed")
    
    erol_output = state.get("erol_gungor_agent_output")
    cemil_output = state.get("cemil_meric_agent_output")
    
    print(f"âœ“ DEBUG: Erol GÃ¼ngÃ¶r output available: {erol_output is not None}")
    print(f"âœ“ DEBUG: Cemil MeriÃ§ output available: {cemil_output is not None}")
    
    # This node doesn't modify state, just serves as a junction
    return {}

def extract_sources_from_messages(messages):
    """Extract sources from agent messages."""
    sources = []
    
    if not messages:
        return sources
    
    for message in messages:
        content = message.content if hasattr(message, 'content') else str(message)
        
        # Extract vector database sources
        if "Kaynak:" in content:
            lines = content.split('\n')
            for line in lines:
                if line.strip().startswith("Kaynak:"):
                    source_name = line.replace("Kaynak:", "").strip()
                    if source_name and source_name not in [s["name"] for s in sources]:
                        sources.append({
                            "type": "vector_db",
                            "name": source_name,
                            "description": f"Kaynak: {source_name}"
                        })
        
        # Extract web search indication
        if any(keyword in content.lower() for keyword in ["web aramasÄ±", "internet", "gÃ¼ncel", "duckduckgo"]):
            web_search_exists = any(s["type"] == "web_search" for s in sources)
            if not web_search_exists:
                sources.append({
                    "type": "web_search", 
                    "name": "Web AramasÄ±",
                    "description": "Ä°nternet aramasÄ± yapÄ±ldÄ±"
                })
    
    return sources

def synthesize_response_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Ä°ki ajanÄ±n yanÄ±tlarÄ±nÄ± birleÅŸtiren node."""
    
    print(f"\nðŸ”„ DEBUG: Starting synthesis node")
    
    # Get the LLM from config
    llm = config.get("configurable", {}).get("llm")
    if not llm:
        print(f"âŒ DEBUG: LLM not found in config for synthesis")
        return {"synthesized_answer": "Sentez iÃ§in dil modeli yapÄ±landÄ±rÄ±lmamÄ±ÅŸ."}
    
    # Extract responses from agent outputs
    erol_response = ""
    cemil_response = ""
    
    # Process Erol GÃ¼ngÃ¶r's output
    erol_output = state.get("erol_gungor_agent_output", {})
    if erol_output and "messages" in erol_output:
        erol_messages = erol_output["messages"]
        if erol_messages:
            last_message = erol_messages[-1]
            erol_response = last_message.content if hasattr(last_message, 'content') else str(last_message)
    elif erol_output and "error" in erol_output:
        erol_response = f"Erol GÃ¼ngÃ¶r yanÄ±tÄ± alÄ±namadÄ±: {erol_output['error']}"
    
    # Process Cemil MeriÃ§'s output
    cemil_output = state.get("cemil_meric_agent_output", {})
    if cemil_output and "messages" in cemil_output:
        cemil_messages = cemil_output["messages"]
        if cemil_messages:
            last_message = cemil_messages[-1]
            cemil_response = last_message.content if hasattr(last_message, 'content') else str(last_message)
    elif cemil_output and "error" in cemil_output:
        cemil_response = f"Cemil MeriÃ§ yanÄ±tÄ± alÄ±namadÄ±: {cemil_output['error']}"
    
    print(f"ðŸ“ DEBUG: Erol GÃ¼ngÃ¶r response length: {len(erol_response)} characters")
    print(f"ðŸ“ DEBUG: Cemil MeriÃ§ response length: {len(cemil_response)} characters")
    
    # Extract sources from both agents
    all_sources = []
    
    # Extract sources from Erol GÃ¼ngÃ¶r's messages
    if erol_output and "messages" in erol_output:
        erol_sources = extract_sources_from_messages(erol_output["messages"])
        all_sources.extend(erol_sources)
        print(f"ðŸ“š DEBUG: Found {len(erol_sources)} sources from Erol GÃ¼ngÃ¶r")
    
    # Extract sources from Cemil MeriÃ§'s messages  
    if cemil_output and "messages" in cemil_output:
        cemil_sources = extract_sources_from_messages(cemil_output["messages"])
        all_sources.extend(cemil_sources)
        print(f"ðŸ“š DEBUG: Found {len(cemil_sources)} sources from Cemil MeriÃ§")
    
    print(f"ðŸ“š DEBUG: Total sources found: {len(all_sources)}")
    
    # Create synthesis prompt
    synthesis_prompt = f"""Sen, TÃ¼rk entelektÃ¼el geleneÄŸini anlayan ve farklÄ± bakÄ±ÅŸ aÃ§Ä±larÄ±nÄ± sentezleyebilen bir asistansÄ±n.

KullanÄ±cÄ± Sorusu: {state['user_query']}

Erol GÃ¼ngÃ¶r'Ã¼n YanÄ±tÄ±:
{erol_response}

Cemil MeriÃ§'in YanÄ±tÄ±:
{cemil_response}

GÃ¶revin: Bu iki entelektÃ¼elin yanÄ±tlarÄ±nÄ± birleÅŸtirerek tek bir tutarlÄ±, kapsamlÄ± yanÄ±t oluÅŸturmak. 

Sentez yaparken:
1. Her iki perspektifi de saygÄ±yla dahil et
2. Ortak noktalarÄ± vurgula
3. FarklÄ± gÃ¶rÃ¼ÅŸleri de belirt ve bunlarÄ± tamamlayÄ±cÄ± olarak sun
4. TekrarlarÄ± Ã¶nle
5. AkÄ±cÄ±, tutarlÄ± bir metin oluÅŸtur
6. Her iki entelektÃ¼elin katkÄ±sÄ±nÄ± acknowledge et

BaÅŸlÄ±klar kullanma, doÄŸrudan kapsamlÄ± bir yanÄ±t ver."""
    
    try:
        print(f"ðŸ§  DEBUG: Sending synthesis request to Gemini...")
        synthesis_result = llm.invoke(synthesis_prompt)
        
        synthesized_text = synthesis_result.content if hasattr(synthesis_result, 'content') else str(synthesis_result)
        print(f"âœ… DEBUG: Synthesis completed successfully")
        print(f"ðŸ“ DEBUG: Synthesized response length: {len(synthesized_text)} characters")
        
        # Prepare individual agent responses for frontend
        agent_responses = {
            "Erol GÃ¼ngÃ¶r": erol_response,
            "Cemil MeriÃ§": cemil_response
        }
        
        print(f"ðŸŽ¯ DEBUG: Agent responses prepared for frontend: {list(agent_responses.keys())}")
        
        return {
            "synthesized_answer": synthesized_text,
            "agent_responses": agent_responses,
            "sources": all_sources
        }
        
    except Exception as e:
        print(f"âŒ DEBUG: Error during synthesis: {str(e)}")
        return {
            "synthesized_answer": f"Sentez hatasÄ±: {str(e)}",
            "agent_responses": {
                "Erol GÃ¼ngÃ¶r": erol_response if erol_response else "YanÄ±t alÄ±namadÄ±",
                "Cemil MeriÃ§": cemil_response if cemil_response else "YanÄ±t alÄ±namadÄ±"
            },
            "sources": all_sources
        }

def update_history_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Sohbet geÃ§miÅŸini gÃ¼ncelleyen node."""
    
    print(f"\nðŸ“š DEBUG: Updating chat history")
    
    # Create messages for history
    user_message = HumanMessage(content=state["user_query"])
    ai_message = AIMessage(content=state.get("synthesized_answer", "YanÄ±t oluÅŸturulamadÄ±"))
    
    print(f"ðŸ’¬ DEBUG: Adding user message and AI response to history")
    print(f"ðŸ“Š DEBUG: Current history length: {len(state.get('chat_history', []))}")
    
    new_messages = [user_message, ai_message]
    
    print(f"âœ… DEBUG: Chat history updated with {len(new_messages)} new messages")
    
    return {"chat_history": new_messages}

# --- Graph Builder ---

class MultiAgentOrchestrator:
    """Multi-agent orchestration system using LangGraph."""
    
    def __init__(self):
        """Orchestrator'Ä± baÅŸlatÄ±r."""
        self.graph = None
        self.qdrant_client = None
        self.embedding_model = None
        self.llm = None
        self.agents = {}
        
    def initialize(self):
        """TÃ¼m bileÅŸenleri baÅŸlatÄ±r."""
        
        print(f"\nðŸš€ DEBUG: Initializing Multi-Agent Orchestrator")
        
        # Initialize Phase 1 components
        print(f"ðŸ”§ DEBUG: Initializing Phase 1 components...")
        self.qdrant_client, self.embedding_model, self.llm = initialize_components()
        
        if not all([self.qdrant_client, self.embedding_model, self.llm]):
            raise Exception("Phase 1 bileÅŸenleri baÅŸlatÄ±lamadÄ±")
        
        print(f"âœ… DEBUG: Phase 1 components initialized successfully")
        
        # Create persona agents
        print(f"ðŸ‘¥ DEBUG: Creating persona agents...")
        available_personas = list_available_personas()
        
        for persona_key in available_personas:
            try:
                agent = create_persona_agent(
                    persona_key, 
                    self.qdrant_client, 
                    self.embedding_model, 
                    self.llm
                )
                self.agents[persona_key] = agent
                persona_info = get_persona_info(persona_key)
                print(f"âœ“ DEBUG: Created agent for {persona_info['name']}")
            except Exception as e:
                print(f"âŒ DEBUG: Failed to create agent for {persona_key}: {e}")
                raise
        
        print(f"âœ… DEBUG: All persona agents created successfully")
        
        # Build the graph
        print(f"ðŸ—ï¸ DEBUG: Building LangGraph workflow...")
        self._build_graph()
        print(f"âœ… DEBUG: Multi-Agent Orchestrator initialized successfully")
    
    def _build_graph(self):
        """LangGraph workflow'unu oluÅŸturur."""
        
        print(f"\nðŸ—ï¸ DEBUG: Building state graph...")
        
        # Create the state graph
        workflow = StateGraph(GraphState)
        
        # Add nodes
        print(f"âž• DEBUG: Adding nodes to graph...")
        workflow.add_node("erol_gungor_agent", erol_gungor_agent_node)
        workflow.add_node("cemil_meric_agent", cemil_meric_agent_node)
        workflow.add_node("join_agents", join_agents_node)
        workflow.add_node("synthesize_response", synthesize_response_node)
        workflow.add_node("update_history", update_history_node)
        
        # Add edges
        print(f"ðŸ”— DEBUG: Adding edges to graph...")
        
        # Entry point: Start with both agents in parallel
        workflow.add_edge(START, "erol_gungor_agent")
        workflow.add_edge(START, "cemil_meric_agent")
        
        # Both agents feed into join node
        workflow.add_edge("erol_gungor_agent", "join_agents")
        workflow.add_edge("cemil_meric_agent", "join_agents")
        
        # Sequential flow after joining
        workflow.add_edge("join_agents", "synthesize_response")
        workflow.add_edge("synthesize_response", "update_history")
        workflow.add_edge("update_history", END)
        
        # Compile the graph
        print(f"âš™ï¸ DEBUG: Compiling graph...")
        checkpointer = MemorySaver()
        self.graph = workflow.compile(checkpointer=checkpointer)
        
        print(f"âœ… DEBUG: Graph compiled successfully")
    
    def invoke(self, user_query: str, thread_id: str = "default") -> Dict[str, Any]:
        """Orchestrator'Ä± Ã§alÄ±ÅŸtÄ±rÄ±r."""
        
        print(f"\nðŸŽ¯ DEBUG: Invoking Multi-Agent Orchestrator")
        print(f"ðŸ’¬ DEBUG: User query: {user_query}")
        print(f"ðŸ§µ DEBUG: Thread ID: {thread_id}")
        
        # Initialize tracing for this session
        session_id = initialize_tracing(thread_id)
        
        # Prepare initial state
        initial_state = {
            "user_query": user_query,
            "erol_gungor_agent_output": None,
            "cemil_meric_agent_output": None,
            "synthesized_answer": None,
            "agent_responses": None,
            "chat_history": [],
            "session_id": session_id,
            "erol_trace_id": None,
            "cemil_trace_id": None
        }
        
        # Prepare config with agents
        config = {
            "configurable": {
                "erol_agent": self.agents.get("erol_gungor"),
                "cemil_agent": self.agents.get("cemil_meric"),
                "llm": self.llm,
                "thread_id": thread_id
            }
        }
        
        try:
            print(f"ðŸš€ DEBUG: Starting graph execution...")
            result = self.graph.invoke(initial_state, config)
            
            print(f"âœ… DEBUG: Graph execution completed successfully")
            print(f"ðŸ“Š DEBUG: Final state keys: {list(result.keys())}")
            
            return result
            
        except Exception as e:
            print(f"âŒ DEBUG: Error during graph execution: {str(e)}")
            return {
                "error": f"Orchestration hatasÄ±: {str(e)}",
                "user_query": user_query,
                "synthesized_answer": "Sistem hatasÄ± nedeniyle yanÄ±t oluÅŸturulamadÄ±."
            }

# --- Convenience Functions ---

# Global orchestrator instance for reuse across requests
_global_orchestrator = None
import threading
_orchestrator_lock = threading.Lock()

def get_global_orchestrator() -> MultiAgentOrchestrator:
    """Get or create the global orchestrator instance (thread-safe)."""
    global _global_orchestrator
    
    with _orchestrator_lock:
        if _global_orchestrator is None:
            print(f"\nðŸ”§ DEBUG: Creating global orchestrator instance (first time initialization)...")
            _global_orchestrator = MultiAgentOrchestrator()
            _global_orchestrator.initialize()
            print(f"âœ… DEBUG: Global orchestrator created and initialized successfully")
        else:
            print(f"â™»ï¸ DEBUG: Reusing existing global orchestrator instance (performance optimized)")
    
    return _global_orchestrator

def create_orchestrator() -> MultiAgentOrchestrator:
    """Yeni bir Multi-Agent Orchestrator oluÅŸturur ve baÅŸlatÄ±r."""
    
    orchestrator = MultiAgentOrchestrator()
    orchestrator.initialize()
    return orchestrator

def run_multi_agent_query(query: str, thread_id: str = "default") -> Dict[str, Any]:
    """Tek seferlik multi-agent sorgu Ã§alÄ±ÅŸtÄ±rÄ±r - now uses global orchestrator for performance."""
    
    # Use the global orchestrator instead of creating a new one each time
    # This dramatically improves performance by avoiding model reloading
    orchestrator = get_global_orchestrator()
    
    return orchestrator.invoke(query, thread_id) 