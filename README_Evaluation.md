# Multi-Agent Persona System Evaluation Pipeline

Bu deÄŸerlendirme pipeline'Ä±, Ã§ok-ajanlÄ± persona sisteminin performansÄ±nÄ± kapsamlÄ± bir ÅŸekilde analiz etmek iÃ§in tasarlanmÄ±ÅŸtÄ±r. RAGAS framework'Ã¼ ve LangChain deÄŸerlendiricilerini kullanarak sistem Ã§Ä±ktÄ±larÄ±nÄ± objektif metriklerle deÄŸerlendirir.

## ğŸ¯ DeÄŸerlendirme Kriterleri

### 1. Faithfulness (Sadakat) - RAGAS
- **AmaÃ§**: YanÄ±tlarÄ±n kaynak metinlere ne kadar sadÄ±k kaldÄ±ÄŸÄ±nÄ± Ã¶lÃ§er
- **Metrik**: RAGAS Faithfulness Score (0-1 arasÄ±)
- **Ã–nemli**: Persona ajanlarÄ±nÄ±n kendi bilgi tabanlarÄ±ndaki kaynaklara ne kadar baÄŸlÄ± kaldÄ±ÄŸÄ±nÄ± gÃ¶sterir

### 2. Answer Relevancy (YanÄ±t Ä°lgililiÄŸi) - RAGAS  
- **AmaÃ§**: YanÄ±tlarÄ±n kullanÄ±cÄ± sorusuna ne kadar uygun olduÄŸunu Ã¶lÃ§er
- **Metrik**: RAGAS Answer Relevancy Score (0-1 arasÄ±)
- **Ã–nemli**: AjanlarÄ±n soruyu doÄŸru anlayÄ±p ilgili yanÄ±t verip vermediÄŸini gÃ¶sterir

### 3. Context Precision (BaÄŸlam Hassasiyeti) - RAGAS
- **AmaÃ§**: AlÄ±nan kaynak metinlerin ne kadar hassas/ilgili olduÄŸunu Ã¶lÃ§er
- **Metrik**: RAGAS Context Precision Score (0-1 arasÄ±)
- **Ã–nemli**: RAG sisteminin doÄŸru kaynaklardan bilgi Ã§ekip Ã§ekmediÄŸini gÃ¶sterir

### 4. Context Recall (BaÄŸlam Geri Ã‡aÄŸÄ±rma) - RAGAS
- **AmaÃ§**: Ä°lgili tÃ¼m kaynak bilgilerinin ne kadarÄ±nÄ±n alÄ±ndÄ±ÄŸÄ±nÄ± Ã¶lÃ§er
- **Metrik**: RAGAS Context Recall Score (0-1 arasÄ±)
- **Ã–nemli**: Sistemin kapsamlÄ± bilgi toplama becerisini gÃ¶sterir

### 5. Coherence (TutarlÄ±lÄ±k) - LangChain
- **AmaÃ§**: YanÄ±tlarÄ±n mantÄ±ksal tutarlÄ±lÄ±ÄŸÄ±nÄ± ve akÄ±cÄ±lÄ±ÄŸÄ±nÄ± Ã¶lÃ§er
- **Metrik**: LangChain Coherence Score (0-1 arasÄ±)
- **Ã–nemli**: SentezlenmiÅŸ yanÄ±tlarÄ±n ne kadar tutarlÄ± olduÄŸunu gÃ¶sterir

## ğŸ—ï¸ Pipeline Mimarisi

### Evaluation Pipeline AdÄ±mlarÄ±

```
1. Sistem Sorgusu
   â”œâ”€ Multi-agent orchestrator Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r
   â”œâ”€ Her iki persona ajanÄ± paralel Ã§alÄ±ÅŸÄ±r
   â””â”€ YanÄ±tlar sentezlenir

2. RAGAS DeÄŸerlendirmesi
   â”œâ”€ Faithfulness hesaplanÄ±r
   â”œâ”€ Answer Relevancy hesaplanÄ±r
   â”œâ”€ Context Precision hesaplanÄ±r
   â””â”€ Context Recall hesaplanÄ±r

3. Coherence DeÄŸerlendirmesi
   â”œâ”€ LangChain evaluator kullanÄ±lÄ±r
   â””â”€ MantÄ±ksal tutarlÄ±lÄ±k Ã¶lÃ§Ã¼lÃ¼r

4. SonuÃ§ Kaydetme
   â”œâ”€ DetaylÄ± JSON raporu
   â”œâ”€ Ã–zet CSV tablosu
   â””â”€ Konsol Ã§Ä±ktÄ±sÄ±
```

## ğŸ“¦ Kurulum

### Gerekli BaÄŸÄ±mlÄ±lÄ±klar

```bash
# Temel baÄŸÄ±mlÄ±lÄ±klar zaten requirements.txt'de mevcut
pip install -r requirements.txt

# Ek deÄŸerlendirme baÄŸÄ±mlÄ±lÄ±klarÄ± (otomatik yÃ¼klenecek)
# - ragas>=0.2.0
# - datasets>=2.14.0  
# - pandas>=2.0.0
# - tabulate>=0.9.0
# - langchain-experimental>=0.0.50
```

### Ortam DeÄŸiÅŸkenleri

```bash
# Google API anahtarÄ± (Gemini iÃ§in gerekli)
export GOOGLE_API_KEY="your-gemini-api-key"
```

## ğŸš€ KullanÄ±m

### 1. Basit DeÄŸerlendirme

```python
from evaluation_pipeline import EvaluationPipeline, EvaluationConfig

# YapÄ±landÄ±rma oluÅŸtur
config = EvaluationConfig(
    test_queries=[
        "TÃ¼rk kÃ¼ltÃ¼rel kimliÄŸi hakkÄ±nda ne dÃ¼ÅŸÃ¼nÃ¼yorsunuz?",
        "ModernleÅŸme ve gelenek arasÄ±ndaki denge nedir?"
    ]
)

# Pipeline Ã§alÄ±ÅŸtÄ±r
pipeline = EvaluationPipeline(config)
results = pipeline.run_evaluation()
pipeline.save_results()
```

### 2. Ä°nteraktif MenÃ¼

```bash
python run_evaluation.py
```

Bu komut ÅŸu seÃ§enekleri sunar:
- **HÄ±zlÄ± DeÄŸerlendirme**: 2 sorgu ile test
- **KÃ¼ltÃ¼rel Kimlik OdaklÄ±**: 3 kÃ¼ltÃ¼rel sorgu
- **KapsamlÄ± DeÄŸerlendirme**: 9 farklÄ± kategoride sorgu
- **Sadece RAGAS**: Daha hÄ±zlÄ± deÄŸerlendirme
- **Ã–zel DeÄŸerlendirme**: Kendi sorularÄ±nÄ±zla

### 3. Programatik KullanÄ±m

```python
# Ã–zel sorular ile deÄŸerlendirme
custom_queries = [
    "BatÄ±lÄ±laÅŸma sÃ¼recinin etkileri nelerdir?",
    "EntelektÃ¼el sorumluluÄŸun sÄ±nÄ±rlarÄ± nedir?"
]

pipeline = EvaluationPipeline(EvaluationConfig(
    test_queries=custom_queries,
    output_dir="my_evaluation",
    enable_ragas=True,
    enable_coherence=True
))

results = pipeline.run_evaluation()
```

## ğŸ“Š Ã‡Ä±ktÄ± FormatlarÄ±

### 1. Konsol Ã‡Ä±ktÄ±sÄ±

Pipeline Ã§alÄ±ÅŸÄ±rken detaylÄ± debug bilgileri gÃ¶sterir:

```
ğŸ¯ EVALUATING QUERY: TÃ¼rk kÃ¼ltÃ¼rel kimliÄŸi hakkÄ±nda ne dÃ¼ÅŸÃ¼nÃ¼yorsunuz?
================================================================================

ğŸ“ STEP 1: Running system query...
ğŸ¯ Starting Erol GÃ¼ngÃ¶r agent node
ğŸ” User query: TÃ¼rk kÃ¼ltÃ¼rel kimliÄŸi hakkÄ±nda ne dÃ¼ÅŸÃ¼nÃ¼yorsunuz?
ğŸ’¬ Invoking Erol GÃ¼ngÃ¶r agent
âœ… Erol GÃ¼ngÃ¶r agent completed successfully

ğŸ“Š STEP 2: RAGAS evaluation...
ğŸ” Running RAGAS evaluation with 5 contexts...
ğŸ“ˆ faithfulness: 0.892
ğŸ“ˆ answer_relevancy: 0.945
ğŸ“ˆ context_precision: 0.823
ğŸ“ˆ context_recall: 0.756

ğŸ§  STEP 3: Coherence evaluation...
ğŸ” Running coherence evaluation...
ğŸ“ˆ Coherence score: 0.867
```

### 2. Ã–zet Tablosu

```
ğŸ“‹ EVALUATION RESULTS SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Query                    Faithfulness  Answer Relevancy  Context Precision  Context Recall  Coherence  Sources  Errors
TÃ¼rk kÃ¼ltÃ¼rel kimliÄŸi... 0.892        0.945            0.823             0.756          0.867      5        0
ModernleÅŸme ve gelenek... 0.834        0.912            0.789             0.823          0.798      4        0
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ AVERAGE SCORES:
  Faithfulness: 0.863 (n=2)
  Answer Relevancy: 0.928 (n=2)
  Context Precision: 0.806 (n=2)
  Context Recall: 0.789 (n=2)
  Coherence: 0.832 (n=2)
```

### 3. DetaylÄ± JSON Raporu

```json
[
  {
    "query": "TÃ¼rk kÃ¼ltÃ¼rel kimliÄŸi hakkÄ±nda ne dÃ¼ÅŸÃ¼nÃ¼yorsunuz?",
    "timestamp": "2024-01-23T15:30:45",
    "erol_response": "Erol GÃ¼ngÃ¶r'Ã¼n yanÄ±tÄ±...",
    "cemil_response": "Cemil MeriÃ§'in yanÄ±tÄ±...",
    "synthesized_response": "SentezlenmiÅŸ final yanÄ±t...",
    "sources": [
      {
        "source": "Erol GÃ¼ngÃ¶r - KÃ¼ltÃ¼r DeÄŸiÅŸmeleri",
        "content": "Kaynak metin iÃ§eriÄŸi..."
      }
    ],
    "faithfulness_score": 0.892,
    "answer_relevancy_score": 0.945,
    "context_precision_score": 0.823,
    "context_recall_score": 0.756,
    "coherence_score": 0.867,
    "coherence_explanation": "YanÄ±t mantÄ±ksal olarak tutarlÄ±...",
    "errors": []
  }
]
```

## ğŸ”§ YapÄ±landÄ±rma SeÃ§enekleri

### EvaluationConfig Parametreleri

```python
@dataclass
class EvaluationConfig:
    test_queries: List[str]           # DeÄŸerlendirilecek sorular
    output_dir: str = "evaluation_results"  # Ã‡Ä±ktÄ± dizini
    save_detailed_results: bool = True      # DetaylÄ± JSON kaydet
    save_summary_table: bool = True         # Ã–zet tablo kaydet
    enable_ragas: bool = True              # RAGAS deÄŸerlendirmesi
    enable_coherence: bool = True          # Coherence deÄŸerlendirmesi
    evaluation_model: str = "gemini-2.0-flash-exp"  # DeÄŸerlendirme modeli
    temperature: float = 0.1               # Model sÄ±caklÄ±ÄŸÄ±
```

### Ã–zel DeÄŸerlendirme SenaryolarÄ±

```python
# Sadece RAGAS (daha hÄ±zlÄ±)
config = EvaluationConfig(
    test_queries=my_queries,
    enable_coherence=False
)

# Sadece Coherence
config = EvaluationConfig(
    test_queries=my_queries,
    enable_ragas=False
)

# Ã–zel Ã§Ä±ktÄ± dizini
config = EvaluationConfig(
    test_queries=my_queries,
    output_dir="my_custom_results"
)
```

## ğŸ“ˆ Metrik Yorumlama

### Skor AralÄ±klarÄ±

- **0.9 - 1.0**: MÃ¼kemmel performans
- **0.8 - 0.9**: Ã‡ok iyi performans  
- **0.7 - 0.8**: Ä°yi performans
- **0.6 - 0.7**: Orta performans
- **0.5 - 0.6**: ZayÄ±f performans
- **0.0 - 0.5**: Ã‡ok zayÄ±f performans

### Metrik BazlÄ± Analiz

#### Faithfulness DÃ¼ÅŸÃ¼kse:
- Ajanlar kaynaklarÄ±ndan sapÄ±yor olabilir
- RAG sistemi yanlÄ±ÅŸ bilgi Ã§ekiyor olabilir
- Persona prompts'larÄ± gÃ¼Ã§lendirilmeli

#### Answer Relevancy DÃ¼ÅŸÃ¼kse:
- Ajanlar soruyu yanlÄ±ÅŸ anlÄ±yor olabilir
- Query processing iyileÅŸtirilmeli
- Persona understanding geliÅŸtirilmeli

#### Context Precision DÃ¼ÅŸÃ¼kse:
- Ã‡ok fazla irrelevant kaynak Ã§ekiliyor
- Semantic search threshold'u artÄ±rÄ±lmalÄ±
- Embedding model kalitesi sorgulanmalÄ±

#### Context Recall DÃ¼ÅŸÃ¼kse:
- Yeterli kaynak Ã§ekilmiyor
- Knowledge base eksik olabilir
- Search limit'i artÄ±rÄ±lmalÄ±

#### Coherence DÃ¼ÅŸÃ¼kse:
- Synthesis logic'i iyileÅŸtirilmeli
- Persona integration problemi var
- Response generation prompts'larÄ± gÃ¼Ã§lendirilmeli

## ğŸ› ï¸ Troubleshooting

### YaygÄ±n Hatalar

#### 1. RAGAS Import HatasÄ±
```bash
pip install ragas datasets
```

#### 2. LangChain Evaluation HatasÄ±
```bash
pip install langchain-experimental
```

#### 3. Google API Key HatasÄ±
```bash
export GOOGLE_API_KEY="your-key-here"
```

#### 4. Qdrant BaÄŸlantÄ± HatasÄ±
- Qdrant server'Ä±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun
- Knowledge base'lerin yÃ¼klendiÄŸini kontrol edin

### Debug ModlarÄ±

Pipeline otomatik olarak detaylÄ± debug bilgileri gÃ¶sterir:
- System query execution
- Agent responses
- RAGAS evaluation steps
- Coherence evaluation process
- Error tracking

## ğŸ”„ Entegrasyon

### Mevcut Sistemle Entegrasyon

Pipeline, mevcut Phase 2 multi-agent orchestrator ile tam uyumludur:
- âœ… LangGraph workflow'u destekler
- âœ… LangSmith tracing ile uyumlu
- âœ… Persona agent'larÄ± kullanÄ±r
- âœ… RAG sistemini deÄŸerlendirir
- âœ… Tool usage'Ä± analiz eder

### CI/CD Pipeline Entegrasyonu

```bash
# Otomatik deÄŸerlendirme scripti
python run_evaluation.py --config quick --output ci_results
```

## ğŸ“ Ã–rnek KullanÄ±m SenaryolarÄ±

### 1. GeliÅŸtirme SÃ¼recinde Kalite KontrolÃ¼

```python
# Her major deÄŸiÅŸiklik sonrasÄ± Ã§alÄ±ÅŸtÄ±r
config = EvaluationConfig(
    test_queries=get_comprehensive_queries(),
    output_dir=f"evaluation_{version}",
    enable_ragas=True,
    enable_coherence=True
)
```

### 2. A/B Testing

```python
# FarklÄ± prompt versiyonlarÄ±nÄ± karÅŸÄ±laÅŸtÄ±r
config_v1 = EvaluationConfig(test_queries=test_set, output_dir="eval_v1")
config_v2 = EvaluationConfig(test_queries=test_set, output_dir="eval_v2")
```

### 3. Performance Monitoring

```python
# DÃ¼zenli performans izleme
config = EvaluationConfig(
    test_queries=monitoring_queries,
    output_dir=f"monitoring_{date}",
    enable_ragas=True
)
```

## ğŸ¯ Gelecek GeliÅŸtirmeler

- [ ] Batch evaluation desteÄŸi
- [ ] Custom metric ekleme
- [ ] Grafik gÃ¶rselleÅŸtirme
- [ ] Automated benchmarking
- [ ] Multi-language support
- [ ] Real-time evaluation API

---

Bu evaluation pipeline, multi-agent persona sisteminizin kalitesini objektif metriklerle Ã¶lÃ§menizi ve sÃ¼rekli iyileÅŸtirmenizi saÄŸlar. DetaylÄ± debug bilgileri ve kapsamlÄ± raporlama ile sistem performansÄ±nÄ±zÄ± derinlemesine analiz edebilirsiniz. 