"""
Phase 2: Multi-Agent Orchestration with LangGraph

Bu modÃ¼l, iki persona ajanÄ±nÄ±n (Erol GÃ¼ngÃ¶r ve Cemil MeriÃ§) paralel Ã§alÄ±ÅŸmasÄ±nÄ± koordine eden
Ã§ok-ajanli orkestrasyon sistemini iÃ§erir. LangGraph kullanarak tool node'larÄ± ve conditional
edge'leri destekler.

Bu system ÅŸu bileÅŸenleri iÃ§erir:
- Graph State: TÃ¼m ajanlarÄ±n durumunu takip eden TypedDict
- Persona Agent Nodes: Her persona iÃ§in ayrÄ± node'lar
- Tool Nodes: Her ajanÄ±n tool Ã§aÄŸrÄ±larÄ±nÄ± yÃ¶neten node'lar
- Synthesizer Node: AjanlarÄ±n Ã§Ä±ktÄ±larÄ±nÄ± birleÅŸtiren node
- Memory Node: Sohbet geÃ§miÅŸini gÃ¼ncelleyen node
- LangSmith Tracing: Comprehensive agent execution tracing
"""

import asyncio
from typing import Any, Dict, List, Optional, TypedDict, Annotated, Literal
from dataclasses import dataclass

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

# Import Phase 1 components
from .persona_agents import initialize_components, create_persona_agent
from .persona_prompts import get_persona_info, list_available_personas

# Import logging
from utils.logging_config import get_orchestrator_logger

# Import LangSmith tracing - using lazy imports to avoid circular dependency
# Note: evaluation imports moved to functions to avoid circular dependency

# Initialize logger
logger = get_orchestrator_logger()

# --- Graph State Definition ---

class GraphState(TypedDict):
    """LangGraph iÃ§in durum yÃ¶netimi."""
    user_query: str
    erol_gungor_agent_output: Optional[Dict[str, Any]]
    cemil_meric_agent_output: Optional[Dict[str, Any]]
    synthesized_answer: Optional[str]
    agent_responses: Optional[Dict[str, str]]
    sources: Optional[List[Dict[str, str]]]
    chat_history: Annotated[List[BaseMessage], add_messages]
    # Agent-specific memory tracking
    erol_gungor_history: Annotated[List[BaseMessage], add_messages]
    cemil_meric_history: Annotated[List[BaseMessage], add_messages]
    # Tracing fields
    session_id: Optional[str]
    erol_trace_id: Optional[str]
    cemil_trace_id: Optional[str]

# LangGraph handles memory automatically with MemorySaver and add_messages
# No need for custom memory management

# --- Node Functions ---

def erol_gungor_agent_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Erol GÃ¼ngÃ¶r ajanÄ±nÄ± Ã§alÄ±ÅŸtÄ±ran node."""
    
    # Lazy import to avoid circular dependency
    from evaluation.langsmith_tracing import trace_agent_execution, update_agent_trace, complete_agent_trace, get_realtime_callback
    
    # Start tracing
    trace_id = trace_agent_execution("Erol GÃ¼ngÃ¶r", state['user_query'])
    
    # Get the agent from config
    erol_agent = config.get("configurable", {}).get("erol_agent")
    if not erol_agent:
        logger.error("Erol GÃ¼ngÃ¶r agent not found in config")
        complete_agent_trace(trace_id, "", "Erol GÃ¼ngÃ¶r ajanÄ± yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")
        return {
            "erol_gungor_agent_output": {"error": "Erol GÃ¼ngÃ¶r ajanÄ± yapÄ±landÄ±rÄ±lmamÄ±ÅŸ"},
            "erol_trace_id": trace_id
        }
    
    try:
        # Update trace status
        update_agent_trace(trace_id, "Sorgu analiz ediliyor...")
        
        # Use agent-specific history but LIMIT IT to prevent context dilution
        erol_specific_history = state.get("erol_gungor_history", [])
        
        # Keep only the last 2 exchanges (4 messages max) to prevent tool instruction dilution
        if len(erol_specific_history) > 4:
            erol_specific_history = erol_specific_history[-4:]
        
        # Add tool usage reminder to the current query to ensure it's always visible
        enhanced_query = f"""ğŸ”§ ARAÃ‡ KULLANIM HATIRLATMASI ğŸ”§
Bu soruya yanÄ±t vermeden Ã¶nce MUTLAKA:
1. internal_knowledge_search_erol_gungor aracÄ±nÄ± kullan
2. Gerekirse web_search aracÄ±nÄ± da kullan

KullanÄ±cÄ± Sorusu: {state["user_query"]}"""
        
        current_message = HumanMessage(content=enhanced_query)
        messages = erol_specific_history + [current_message]
        
        # Update trace for agent invocation
        update_agent_trace(trace_id, "Ajan Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        # Get real-time callback for this agent
        callback = get_realtime_callback("Erol GÃ¼ngÃ¶r", trace_id)
        
        # Invoke the agent with real-time callback (if available)
        if callback:
            result = erol_agent.invoke({"messages": messages}, config={"callbacks": [callback]})
        else:
            result = erol_agent.invoke({"messages": messages})
        
        # Extract response for tracing and history update
        response_text = ""
        if result and "messages" in result and result["messages"]:
            last_message = result["messages"][-1]
            response_text = last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        # Complete trace
        complete_agent_trace(trace_id, response_text)
        
        # Update agent-specific history with ORIGINAL user query (not enhanced) and agent response
        original_message = HumanMessage(content=state["user_query"])
        agent_response = AIMessage(content=response_text) if response_text else AIMessage(content="YanÄ±t alÄ±namadÄ±")
        
        return {
            "erol_gungor_agent_output": result,
            "erol_trace_id": trace_id,
            "erol_gungor_history": [original_message, agent_response]  # Store original query in history
        }
        
    except Exception as e:
        logger.error(f"Error in Erol GÃ¼ngÃ¶r agent node: {str(e)}")
        complete_agent_trace(trace_id, "", str(e))
        
        # Update history even on error
        original_message = HumanMessage(content=state["user_query"])
        error_response = AIMessage(content=f"Erol GÃ¼ngÃ¶r ajanÄ± hatasÄ±: {str(e)}")
        
        return {
            "erol_gungor_agent_output": {"error": f"Erol GÃ¼ngÃ¶r ajanÄ± hatasÄ±: {str(e)}"},
            "erol_trace_id": trace_id,
            "erol_gungor_history": [original_message, error_response]
        }

def cemil_meric_agent_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Cemil MeriÃ§ ajanÄ±nÄ± Ã§alÄ±ÅŸtÄ±ran node."""
    
    # Lazy import to avoid circular dependency
    from evaluation.langsmith_tracing import trace_agent_execution, update_agent_trace, complete_agent_trace, get_realtime_callback
    
    # Start tracing
    trace_id = trace_agent_execution("Cemil MeriÃ§", state['user_query'])
    
    # Get the agent from config
    cemil_agent = config.get("configurable", {}).get("cemil_agent")
    if not cemil_agent:
        logger.error("Cemil MeriÃ§ agent not found in config")
        complete_agent_trace(trace_id, "", "Cemil MeriÃ§ ajanÄ± yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")
        return {
            "cemil_meric_agent_output": {"error": "Cemil MeriÃ§ ajanÄ± yapÄ±landÄ±rÄ±lmamÄ±ÅŸ"},
            "cemil_trace_id": trace_id
        }
    
    try:
        # Update trace status
        update_agent_trace(trace_id, "Felsefi Ã§erÃ§eve oluÅŸturuluyor...")
        
        # Use agent-specific history but LIMIT IT to prevent context dilution
        cemil_specific_history = state.get("cemil_meric_history", [])
        
        # Keep only the last 2 exchanges (4 messages max) to prevent tool instruction dilution
        if len(cemil_specific_history) > 4:
            cemil_specific_history = cemil_specific_history[-4:]
        
        # Add tool usage reminder to the current query to ensure it's always visible
        enhanced_query = f"""ğŸ”§ ARAÃ‡ KULLANIM HATIRLATMASI ğŸ”§
Bu soruya yanÄ±t vermeden Ã¶nce MUTLAKA:
1. internal_knowledge_search_cemil_meric aracÄ±nÄ± kullan
2. Gerekirse web_search aracÄ±nÄ± da kullan

KullanÄ±cÄ± Sorusu: {state["user_query"]}"""
        
        current_message = HumanMessage(content=enhanced_query)
        messages = cemil_specific_history + [current_message]
        
        # Update trace for agent invocation
        update_agent_trace(trace_id, "Ajan Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        # Get real-time callback for this agent
        callback = get_realtime_callback("Cemil MeriÃ§", trace_id)
        
        # Invoke the agent with real-time callback (if available)
        if callback:
            result = cemil_agent.invoke({"messages": messages}, config={"callbacks": [callback]})
        else:
            result = cemil_agent.invoke({"messages": messages})
        
        # Extract response for tracing and history update
        response_text = ""
        if result and "messages" in result and result["messages"]:
            last_message = result["messages"][-1]
            response_text = last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        # Complete trace
        complete_agent_trace(trace_id, response_text)
        
        # Update agent-specific history with ORIGINAL user query (not enhanced) and agent response
        original_message = HumanMessage(content=state["user_query"])
        agent_response = AIMessage(content=response_text) if response_text else AIMessage(content="YanÄ±t alÄ±namadÄ±")
        
        return {
            "cemil_meric_agent_output": result,
            "cemil_trace_id": trace_id,
            "cemil_meric_history": [original_message, agent_response]  # Store original query in history
        }
        
    except Exception as e:
        logger.error(f"Error in Cemil MeriÃ§ agent node: {str(e)}")
        complete_agent_trace(trace_id, "", str(e))
        
        # Update history even on error
        original_message = HumanMessage(content=state["user_query"])
        error_response = AIMessage(content=f"Cemil MeriÃ§ ajanÄ± hatasÄ±: {str(e)}")
        
        return {
            "cemil_meric_agent_output": {"error": f"Cemil MeriÃ§ ajanÄ± hatasÄ±: {str(e)}"},
            "cemil_trace_id": trace_id,
            "cemil_meric_history": [original_message, error_response]
        }

def join_agents_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Ä°ki ajanÄ±n tamamlanmasÄ±nÄ± bekleyen ara node."""
    
    erol_output = state.get("erol_gungor_agent_output")
    cemil_output = state.get("cemil_meric_agent_output")
    
    # This node doesn't modify state, just serves as a junction
    return {}

def extract_sources_from_messages(messages, agent_name=None):
    """Extract sources from agent messages with agent attribution."""
    sources = []
    
    if not messages:
        return sources
    
    for message in messages:
        content = message.content if hasattr(message, 'content') else str(message)
        
        # Extract vector database sources
        if "Kaynak:" in content:
            lines = content.split('\n')
            for line in lines:
                if line.strip().startswith("Kaynak:"):
                    source_name = line.replace("Kaynak:", "").strip()
                    if source_name and source_name not in [s["name"] for s in sources]:
                        sources.append({
                            "type": "vector_db",
                            "name": source_name,
                            "description": f"Kaynak: {source_name}",
                            "agent": agent_name
                        })
        
        # Extract web search indication
        if any(keyword in content.lower() for keyword in ["web aramasÄ±", "internet", "gÃ¼ncel", "duckduckgo"]):
            web_search_exists = any(s["type"] == "web_search" and s.get("agent") == agent_name for s in sources)
            if not web_search_exists:
                sources.append({
                    "type": "web_search", 
                    "name": "Web AramasÄ±",
                    "description": "Ä°nternet aramasÄ± yapÄ±ldÄ±",
                    "agent": agent_name
                })
    
    return sources

def synthesize_response_node(state: GraphState, config: RunnableConfig) -> Dict[str, Any]:
    """Ä°ki ajanÄ±n yanÄ±tlarÄ±nÄ± birleÅŸtiren node."""
    
    logger.info("Starting synthesis node")
    
    # Get the LLM from config
    llm = config.get("configurable", {}).get("llm")
    if not llm:
        logger.error("LLM not found in config for synthesis")
        return {"synthesized_answer": "Sentez iÃ§in dil modeli yapÄ±landÄ±rÄ±lmamÄ±ÅŸ."}
    
    # Extract responses from agent outputs
    erol_response = ""
    cemil_response = ""
    
    # Process Erol GÃ¼ngÃ¶r's output
    erol_output = state.get("erol_gungor_agent_output", {})
    if erol_output and "messages" in erol_output:
        erol_messages = erol_output["messages"]
        if erol_messages:
            last_message = erol_messages[-1]
            erol_response = last_message.content if hasattr(last_message, 'content') else str(last_message)
    elif erol_output and "error" in erol_output:
        erol_response = f"Erol GÃ¼ngÃ¶r yanÄ±tÄ± alÄ±namadÄ±: {erol_output['error']}"
    
    # Process Cemil MeriÃ§'s output
    cemil_output = state.get("cemil_meric_agent_output", {})
    if cemil_output and "messages" in cemil_output:
        cemil_messages = cemil_output["messages"]
        if cemil_messages:
            last_message = cemil_messages[-1]
            cemil_response = last_message.content if hasattr(last_message, 'content') else str(last_message)
    elif cemil_output and "error" in cemil_output:
        cemil_response = f"Cemil MeriÃ§ yanÄ±tÄ± alÄ±namadÄ±: {cemil_output['error']}"
    
    # Extract sources from both agents
    all_sources = []
    
    # Extract sources from Erol GÃ¼ngÃ¶r's messages
    if erol_output and "messages" in erol_output:
        erol_sources = extract_sources_from_messages(erol_output["messages"], "Erol GÃ¼ngÃ¶r")
        all_sources.extend(erol_sources)
    
    # Extract sources from Cemil MeriÃ§'s messages  
    if cemil_output and "messages" in cemil_output:
        cemil_sources = extract_sources_from_messages(cemil_output["messages"], "Cemil MeriÃ§")
        all_sources.extend(cemil_sources)
    
    # Create synthesis prompt with chat history context if available
    chat_history = state.get("chat_history", [])
    history_context = ""
    
    if len(chat_history) > 2:  # If there's meaningful chat history
        history_context = "\n\nÃ–nceki Sohbet BaÄŸlamÄ±:\n"
        for i, msg in enumerate(chat_history[-4:]):  # Last 2 exchanges
            role = "KullanÄ±cÄ±" if isinstance(msg, HumanMessage) else "Asistan"
            content = msg.content[:200] + "..." if len(msg.content) > 200 else msg.content
            history_context += f"{role}: {content}\n"
        history_context += "\nBu baÄŸlamÄ± gÃ¶z Ã¶nÃ¼nde bulundurarak yanÄ±t ver.\n"
    
    synthesis_prompt = f"""Sen, TÃ¼rk entelektÃ¼el geleneÄŸini anlayan ve farklÄ± bakÄ±ÅŸ aÃ§Ä±larÄ±nÄ± sentezleyebilen bir asistansÄ±n.
{history_context}
KullanÄ±cÄ± Sorusu: {state['user_query']}

Erol GÃ¼ngÃ¶r'Ã¼n YanÄ±tÄ±:
{erol_response}

Cemil MeriÃ§'in YanÄ±tÄ±:
{cemil_response}

GÃ¶revin: Bu iki entelektÃ¼elin yanÄ±tlarÄ±nÄ± birleÅŸtirerek tek bir tutarlÄ±, kapsamlÄ± yanÄ±t oluÅŸturmak. 

Sentez yaparken:
1. Her iki perspektifi de saygÄ±yla dahil et
2. Ortak noktalarÄ± vurgula
3. FarklÄ± gÃ¶rÃ¼ÅŸleri de belirt ve bunlarÄ± tamamlayÄ±cÄ± olarak sun
4. TekrarlarÄ± Ã¶nle
5. AkÄ±cÄ±, tutarlÄ± bir metin oluÅŸtur
6. Her iki entelektÃ¼elin katkÄ±sÄ±nÄ± acknowledge et
7. EÄŸer Ã¶nceki sohbet baÄŸlamÄ± varsa, ona uygun ÅŸekilde yanÄ±t ver

BaÅŸlÄ±klar kullanma, doÄŸrudan kapsamlÄ± bir yanÄ±t ver."""
    
    try:
        synthesis_result = llm.invoke(synthesis_prompt)
        
        synthesized_text = synthesis_result.content if hasattr(synthesis_result, 'content') else str(synthesis_result)
        logger.info("Synthesis completed successfully")
        
        # Prepare individual agent responses for frontend
        agent_responses = {
            "Erol GÃ¼ngÃ¶r": erol_response,
            "Cemil MeriÃ§": cemil_response
        }
        
        # Update chat history with user query and AI response using LangGraph's add_messages
        user_message = HumanMessage(content=state['user_query'])
        ai_message = AIMessage(content=synthesized_text)
        
        return {
            "synthesized_answer": synthesized_text,
            "agent_responses": agent_responses,
            "sources": all_sources,
            "chat_history": [user_message, ai_message]  # LangGraph will add these to existing history
        }
        
    except Exception as e:
        logger.error(f"Error during synthesis: {str(e)}")
        # Even on error, update chat history
        error_response = f"Sentez hatasÄ±: {str(e)}"
        user_message = HumanMessage(content=state['user_query'])
        ai_message = AIMessage(content=error_response)
        
        return {
            "synthesized_answer": error_response,
            "agent_responses": {
                "Erol GÃ¼ngÃ¶r": erol_response if erol_response else "YanÄ±t alÄ±namadÄ±",
                "Cemil MeriÃ§": cemil_response if cemil_response else "YanÄ±t alÄ±namadÄ±"
            },
            "sources": all_sources,
            "chat_history": [user_message, ai_message]  # LangGraph will add these to existing history
        }

# LangGraph's add_messages annotation automatically handles chat history updates
# No need for manual memory update node

# --- Graph Builder ---

class MultiAgentOrchestrator:
    """Multi-agent orchestration system using LangGraph."""
    
    def __init__(self):
        """Orchestrator'Ä± baÅŸlatÄ±r."""
        self.graph = None
        self.qdrant_client = None
        self.embedding_model = None
        self.llm = None
        self.agents = {}
        
    def initialize(self):
        """TÃ¼m bileÅŸenleri baÅŸlatÄ±r."""
        
        logger.info("Initializing Multi-Agent Orchestrator")
        
        # Initialize Phase 1 components
        self.qdrant_client, self.embedding_model, self.llm = initialize_components()
        
        if not all([self.qdrant_client, self.embedding_model, self.llm]):
            raise Exception("Phase 1 bileÅŸenleri baÅŸlatÄ±lamadÄ±")
        
        logger.info("Phase 1 components initialized successfully")
        
        # Create persona agents
        available_personas = list_available_personas()
        
        for persona_key in available_personas:
            try:
                agent = create_persona_agent(
                    persona_key, 
                    self.qdrant_client, 
                    self.embedding_model, 
                    self.llm
                )
                self.agents[persona_key] = agent
                persona_info = get_persona_info(persona_key)
                logger.info(f"Created agent for {persona_info['name']}")
            except Exception as e:
                logger.error(f"Failed to create agent for {persona_key}: {e}")
                raise
        
        logger.info("All persona agents created successfully")
        
        # Build the graph
        self._build_graph()
        logger.info("Multi-Agent Orchestrator initialized successfully")
    
    def _build_graph(self):
        """LangGraph workflow'unu oluÅŸturur."""
        
        # Create the state graph
        workflow = StateGraph(GraphState)
        
        # Add nodes
        workflow.add_node("erol_gungor_agent", erol_gungor_agent_node)
        workflow.add_node("cemil_meric_agent", cemil_meric_agent_node)
        workflow.add_node("join_agents", join_agents_node)
        workflow.add_node("synthesize_response", synthesize_response_node)
        
        # Add edges
        # Entry point: Start with both agents in parallel
        workflow.add_edge(START, "erol_gungor_agent")
        workflow.add_edge(START, "cemil_meric_agent")
        
        # Both agents feed into join node
        workflow.add_edge("erol_gungor_agent", "join_agents")
        workflow.add_edge("cemil_meric_agent", "join_agents")
        
        # Sequential flow after joining
        workflow.add_edge("join_agents", "synthesize_response")
        workflow.add_edge("synthesize_response", END)  # End directly after synthesis
        
        # Compile the graph
        checkpointer = MemorySaver()
        self.graph = workflow.compile(checkpointer=checkpointer)
        
        logger.info("Graph compiled successfully")
    
    def invoke(self, user_query: str, thread_id: str = "default") -> Dict[str, Any]:
        """Orchestrator'Ä± Ã§alÄ±ÅŸtÄ±rÄ±r using LangGraph's built-in memory management."""
        
        logger.info(f"Invoking Multi-Agent Orchestrator for thread: {thread_id}")
        
        # Initialize tracing for this session
        from evaluation.langsmith_tracing import initialize_tracing
        session_id = initialize_tracing(thread_id)
        
        # Prepare initial state - LangGraph's MemorySaver will handle chat history persistence
        initial_state = {
            "user_query": user_query,
            "erol_gungor_agent_output": None,
            "cemil_meric_agent_output": None,
            "synthesized_answer": None,
            "agent_responses": None,
            "session_id": session_id,
            "erol_trace_id": None,
            "cemil_trace_id": None
        }
        
        # Prepare config with agents for runtime
        runtime_config = {
            "configurable": {
                "erol_agent": self.agents.get("erol_gungor"),
                "cemil_agent": self.agents.get("cemil_meric"),
                "llm": self.llm,
                "thread_id": thread_id
            }
        }
        
        try:
            # LangGraph's MemorySaver automatically loads and saves chat history based on thread_id
            # The thread_id is passed in the config for the checkpointer
            result = self.graph.invoke(
                initial_state, 
                config={"configurable": {"thread_id": thread_id}, **runtime_config}
            )
            logger.info("Graph execution completed successfully")
            return result
            
        except Exception as e:
            logger.error(f"Error during graph execution: {str(e)}")
            return {
                "error": f"Orchestration hatasÄ±: {str(e)}",
                "user_query": user_query,
                "synthesized_answer": "Sistem hatasÄ± nedeniyle yanÄ±t oluÅŸturulamadÄ±."
            }

# --- Convenience Functions ---

# Global orchestrator instance for reuse across requests
_global_orchestrator = None
import threading
_orchestrator_lock = threading.Lock()

def get_global_orchestrator() -> MultiAgentOrchestrator:
    """Get or create the global orchestrator instance (thread-safe)."""
    global _global_orchestrator
    
    with _orchestrator_lock:
        if _global_orchestrator is None:
            logger.info("Creating global orchestrator instance")
            _global_orchestrator = MultiAgentOrchestrator()
            _global_orchestrator.initialize()
            logger.info("Global orchestrator created and initialized successfully")
    
    return _global_orchestrator

def create_orchestrator() -> MultiAgentOrchestrator:
    """Yeni bir Multi-Agent Orchestrator oluÅŸturur ve baÅŸlatÄ±r."""
    
    orchestrator = MultiAgentOrchestrator()
    orchestrator.initialize()
    return orchestrator

def run_multi_agent_query(query: str, thread_id: str = "default") -> Dict[str, Any]:
    """Tek seferlik multi-agent sorgu Ã§alÄ±ÅŸtÄ±rÄ±r - uses LangGraph's built-in memory management."""
    
    # Use the global orchestrator instead of creating a new one each time
    # This dramatically improves performance by avoiding model reloading
    orchestrator = get_global_orchestrator()
    
    return orchestrator.invoke(query, thread_id) 